import os
import sys
import chromadb
from chromadb.utils import embedding_functions
from openai import OpenAI

# Configuration
GROQ_BASE_URL = "https://api.groq.com/openai/v1"
GROQ_MODEL = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

RAG_TEMPLATE = """CONTEXTE:
{context}

QUESTION:
{question}

INSTRUCTIONS:
R√©ponds en te basant UNIQUEMENT sur le contexte fourni. Si tu ne trouves pas la r√©ponse dis "Je ne trouve pas cette information dans les documents".
"""

# Initialisation de ChromaDB (en m√©moire pour ce test, ou persistant si besoin)
# Pour ce test simple, on va cr√©er une petite base temporaire
def setup_chroma():
    client = chromadb.Client()
    # Utilisation d'un mod√®le d'embedding par d√©faut (sentence-transformers/all-MiniLM-L6-v2)
    # Note: Cela t√©l√©chargera le mod√®le au premier lancement
    ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")
    
    collection = client.create_collection(name="tickets_knowledge", embedding_function=ef)
    
    # Ajout de documents de test "Maroc Telecom"
    documents = [
        "Pour les probl√®mes de connexion Maroc Telecom (IAM), v√©rifiez d'abord que le routeur Livebox n'a pas de voyant rouge clignotant.",
        "Si la connexion fibre optique est lente, red√©marrez l'ONT (le petit bo√Ætier blanc) avant le routeur.",
        "Le support technique Maroc Telecom est joignable au 115 pour les particuliers et 124 pour les pro.",
        "Pour configurer l'APN 4G IAM sur mobile: Nom=IAM, APN=www.iam.net.ma, le reste par d√©faut.",
        "En cas de coupure g√©n√©ralis√©e dans le quartier, attendre 1h avant d'appeler le support."
    ]
    
    ids = [f"doc_{i}" for i in range(len(documents))]
    
    collection.add(
        documents=documents,
        ids=ids
    )
    return collection

def ask_bot(query, collection, client_llm):
    print(f"\nüîç Question: {query}")
    
    # √âtape A : Chercher dans la base Chroma
    # On r√©cup√®re les 3 documents les plus proches
    results = collection.query(
        query_texts=[query],
        n_results=3
    )
    
    documents_trouves = results['documents'][0]
    context_text = "\n\n".join(documents_trouves)
    print(f"üìÑ Contexte trouv√© ({len(documents_trouves)} docs):\n{context_text[:200]}...") # Aper√ßu
    
    # √âtape B : Construire le prompt
    prompt = RAG_TEMPLATE.format(context=context_text, question=query)
    
    # √âtape C : Envoyer au LLM
    try:
        response = client_llm.chat.completions.create(
            model=GROQ_MODEL,
            messages=[
                {"role": "system", "content": "Tu es un assistant support expert."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0
        )
        answer = response.choices[0].message.content
        return answer
        
    except Exception as e:
        return f"Erreur LLM: {e}"

def main():
    if not GROQ_API_KEY:
        print("Erreur: GROQ_API_KEY manquant.")
        sys.exit(1)

    print("‚è≥ Initialisation de la base de connaissances (ChromaDB)...")
    collection = setup_chroma()
    
    client_llm = OpenAI(base_url=GROQ_BASE_URL, api_key=GROQ_API_KEY)
    
    # Test
    query = "Comment r√©soudre un probl√®me de connexion Maroc Telecom?"
    reponse = ask_bot(query, collection, client_llm)
    
    print(f"\nü§ñ R√©ponse du Bot:\n{reponse}\n")

if __name__ == "__main__":
    main()
